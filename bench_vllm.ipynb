{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e566079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "\n",
    "import nvidia\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd566719",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fb8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9bafe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70d822996974d71a224a4a04afcca03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "You are using a model of type RefinedWebModel to instantiate a model of type falcon. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-18 20:32:26 llm_engine.py:70] Initializing an LLM engine with config: model='/mnt/falcon_7b_model_adapter', tokenizer='/mnt/falcon_7b_model_adapter', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-18 20:32:45 llm_engine.py:196] # GPU blocks: 43795, # CPU blocks: 32768\n"
     ]
    }
   ],
   "source": [
    "# Load the model to vLLM.\n",
    "llm = LLM(model=\"/mnt/falcon_7b_model_adapter\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1fc1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN ðŸ™Š Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting ðŸ˜± To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on ðŸ˜‚\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: ðŸ™‰\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan ðŸ˜‚\n",
      "Hilary: ðŸ˜Ž\n",
      "Elliot: See you at 2 then xx\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a random test sample\n",
    "# sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# Change the index to select a different sample\n",
    "sample = test_dataset[5]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783a8035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hilary is going to lunch. Benjamin might pass by after lunchtime to take the keys from Hilary. Hilary is meeting French people to talk about colonial food and she is going to La Cantina for Italian cuisine afterwards. Elliot will meet their French guests'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 50\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "start_time = time.perf_counter()\n",
    "outputs = llm.generate(test_sample, sampling_params)\n",
    "end_time = time.perf_counter()\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(f\"{generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c723ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " vLLM took 1.781 sec and generated 28.076  tokens/sec\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n vLLM took {round(end_time - start_time, 3)} sec and generated {round(max_tokens / (end_time - start_time),3)}  tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "outputs = llm.generate(test_sample, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda80cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
