{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f325aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "import argparse\n",
    "import ctranslate2\n",
    "import json\n",
    "import nvidia\n",
    "import os\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig\n",
    "from random import randint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a Huggingface transformers model to a ctranslate model for fast inference\n",
    "\n",
    "def convert_qlora2ct2(adapter_path: str = \"/mnt/falcon_7b_8bit_lora_outputs/checkpoint-3683\",\n",
    "                      offload_path:str =\"/mnt/ct2offload/\",\n",
    "                      full_model_path:str=\"/mnt/falcon_7b_model_adapter\",\n",
    "                      ct2_path:str=\"/mnt/ct2_int8\",\n",
    "                      quantization:str=\"int8\"):\n",
    "\n",
    "    # Load the LLM and its adapter\n",
    "    peft_model_id = adapter_path\n",
    "    peftconfig = PeftConfig.from_pretrained(peft_model_id)\n",
    "    base_model_name_or_path = peftconfig.base_model_name_or_path\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name_or_path, \n",
    "                                                device_map = \"auto\", \n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                trust_remote_code=True,\n",
    "                                                offload_folder  = offload_path,\n",
    "                                                resume_download=True,\n",
    "                                                cache_dir='/mnt',\n",
    "                                                local_files_only=False,\n",
    "                                                )\n",
    "\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(peftconfig.base_model_name_or_path)\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, peft_model_id, device_map='auto')\n",
    "\n",
    "    print(\"Peft model loaded\")\n",
    "    \n",
    "    # Merge the LLM and the adapter\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save the merged model and the adapter\n",
    "    merged_model.save_pretrained(full_model_path) \n",
    "    tokenizer.save_pretrained(full_model_path)\n",
    "\n",
    "    # Convert the HF model to ctranslate\n",
    "    if quantization == False:\n",
    "        os.system(f\"sudo ct2-transformers-converter --model {full_model_path} --output_dir {ct2_path} --trust_remote_code --force\")\n",
    "    else:\n",
    "        os.system(f\"sudo ct2-transformers-converter --model {full_model_path} --output_dir {ct2_path} --quantization {quantization} --trust_remote_code --force\")\n",
    "    print(\" Model Converted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to convert the fine tuned Falcon-7b model to a ctranslate model\n",
    "convert_qlora2ct2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf01d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the converted model and the tokenizer\n",
    "generator = ctranslate2.Generator(\"/mnt/ct2_int8\", device=\"cuda\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the test sample\n",
    "start_time = time.time()\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(test_sample))\n",
    "\n",
    "results = generator.generate_batch([tokens], sampling_topk=10, max_length=200, include_prompt_in_result=False)\n",
    "output = tokenizer.decode(results[0].sequences_ids[0])\n",
    "end_time = time.time()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n Generating the summary took {round(end_time - start_time, 3)} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
